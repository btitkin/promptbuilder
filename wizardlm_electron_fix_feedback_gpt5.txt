Nazwa pliku: wizardlm_electron_fix_feedback.txt
Data: 2025-09-13
Temat: WizardLM 7B + Electron â€“ generateRandomDescription zawsze uÅ¼ywa fallbacku (diagnoza i poprawki)

Streszczenie
- Przyczyna 1 (krytyczna): Stop sequences zawierajÄ… "ASSISTANT:", a prompt koÅ„czy siÄ™ na "ASSISTANT:". Gdy model echem wypisze â€žASSISTANT:â€ lub gdy post-trim szuka â€žASSISTANT:â€, wynik bywa przycinany do pustki â†’ fallback.
- Przyczyna 2: W generateRandomDescription odrzucasz sensowne odpowiedzi â€œheurystykÄ…â€ (hasNoProblematicStart), zanim trafiÄ… do normalizera.
- Przyczyna 3: Normalizator potrafi nadmiernie ingerowaÄ‡ (np. zamiany znakÃ³w/rozbicia separatorÃ³w), co w niektÃ³rych przypadkach wyjaÅ‚awia wynik.

NajwaÅ¼niejsze zmiany (TL;DR)
1) UsuÅ„ "ASSISTANT:" ze stop sequences wszÄ™dzie (renderer i main).
2) ZakoÅ„cz prompt dokÅ‚adnie na "ASSISTANT: " (z trailing space). Zatrzymuj generacjÄ™ na "USER:" (rÃ³Å¼ne warianty z newline).
3) W generateRandomDescription najpierw normalizuj odpowiedÅº i uÅ¼yj jej, a dopiero gdy normalizer zwrÃ³ci pustkÄ™ â€” zastosuj fallback (usuÅ„ pre-filtrowanie â€œhasNoProblematicStartâ€).
4) UÅ‚agodÅº normalizeToTagsLine tak, by nie psuÅ‚ liczb/ekspozycji (np. f/1.8, 1/125s) i nie wycinaÅ‚ zbyt agresywnie.
5) W main.js trimuj ewentualnie tylko po USER:, nie po ASSISTANT:.

Odpowiedzi na Twoje pytania
1) Format promptu TheBloke vs WizardLM (Wizard-Vicuna):
   - Tak, format USER: ... ASSISTANT: jest wÅ‚aÅ›ciwy dla wizard-vicuna-7b-uncensored.
   - Kluczowe: NIE uÅ¼ywaj "ASSISTANT:" w stop sequences. KoÅ„cz prompt na "ASSISTANT: " i dodaj stop'y dla USER: (w tym warianty z nowÄ… liniÄ…).
   - JeÅ›li biblioteka potrafi uÅ¼yÄ‡ chat_template z GGUF, to lepiej. W przeciwnym razie TwÃ³j buildTheBlokePrompt jest OK (dodaj trailing space).

2) Czy normalizeToTagsLine jest zbyt agresywna?
   - Tak, w kilku miejscach moÅ¼e dewastowaÄ‡ wyjÅ›cie (np. zamiana kropek na przecinki, usuwanie slashy). ZmieÅ„ na Å‚agodniejszy wariant (poniÅ¼ej).

3) Jak poprawnie debugowaÄ‡ lokalne LLM?
   - Loguj: kompletny prompt, raw output (przed trymowaniem), post-trim output, stop reason (o ile dostÄ™pny), dÅ‚ugoÅ›ci tekstu przed/po clean/normalize.
   - Testuj normalizer na przykÅ‚adowych odpowiedziach offline.
   - Nie tnij po â€œASSISTANT:â€. Stop tylko na â€œUSER:â€.

Konkretne zmiany w kodzie (kopiuj-wklej)

A) customApiService.ts

1) Stop sequences â€“ usuÅ„ ASSISTANT:
ZmieÅ„:
  const STOP_SEQS = ['USER:', '\nUSER:', 'ASSISTANT:', '\nASSISTANT:'];
Na:
  const STOP_SEQS = ['USER:', '\nUSER:', '\n\nUSER:'];

2) buildTheBlokePrompt â€“ dodaj spacjÄ™ po "ASSISTANT:"
ZmieÅ„:
  return `${preamble}\n\nUSER: ${userText}\nASSISTANT:`;
Na:
  return `${preamble}\n\nUSER: ${userText}\nASSISTANT: `;

3) normalizeToTagsLine â€“ Å‚agodniejsza wersja (peÅ‚na podmiana funkcji):

  const normalizeToTagsLine = (text: string): string => {
    console.log('ðŸ” DEBUG: normalizeToTagsLine input:', JSON.stringify(text));
    
    let s = cleanTextResponse(text);
    console.log('ðŸ” DEBUG: After cleanTextResponse:', JSON.stringify(s));
    console.log('ðŸ” DEBUG: raw->len', text?.length, 'clean->len', s?.length);

    // UsuÅ„ proste etykiety na poczÄ…tku
    s = s.replace(/^\s*(tags?|image tags?|prompt|description)\s*:\s*/i, '');
    s = s.replace(/^(i(?:'|â€™)?ll create|i can suggest|let me generate).*?:\s*/i, '');

    // Zostaw slashy i kropki (nie psuj f/1.8, 1/125s itp.)
    // Konwersja separatorÃ³w: ; | i nowe linie â†’ przecinek
    s = s.replace(/[;|]/g, ',');
    s = s.replace(/\r?\n+/g, ',');
    // UsuÅ„ bullet/list markers
    s = s.replace(/(^|,)\s*(?:KATEX_INLINE_OPEN?\d+KATEX_INLINE_CLOSE?[.)]|[-*â€¢])\s+/g, '$1');

    // UsuÅ„ role tokens
    s = s.replace(/\b(?:OOC|SYS|SYSTEM|USER|ASSISTANT)\b\s*:?/gi, '');

    // UsuÅ„ cudzysÅ‚owy/backticki
    s = s.replace(/["'`]/g, '');

    // Redukcja wielokrotnych przecinkÃ³w/spacji
    s = s.replace(/\s*,\s*/g, ',').replace(/,{2,}/g, ',').replace(/\s{2,}/g, ' ').trim();

    console.log('ðŸ” DEBUG: After basic cleaning:', JSON.stringify(s));

    const rawParts = s.split(',').map(t => t.trim()).filter(Boolean);

    // Delikatne odciÄ™cie oczywistych prefiksÃ³w, ale nie naruszaj wartoÅ›ci technicznych
    const parts = rawParts
      .map(p => p.replace(/^(woman|man|pose|location|clothing|lighting|style|age|build)\s*:?\s*/i, '').trim())
      .filter(Boolean);

    const seen = new Set<string>();
    const tags: string[] = [];
    for (const p of parts) {
      const key = p.toLowerCase();
      if (!seen.has(key)) {
        seen.add(key);
        tags.push(p);
      }
      if (tags.length >= 30) break;
    }

    console.log('ðŸ” DEBUG: Final tags:', tags);
    if (tags.length === 0) {
      console.log('ðŸ” DEBUG: No tags found, using fallback');
      return 'portrait, single subject, natural light, shallow depth of field, cinematic lighting, high detail, sharp focus, photorealistic';
    }

    const result = tags.join(', ');
    console.log('ðŸ” DEBUG: normalizeToTagsLine result:', JSON.stringify(result));
    return result;
  };

4) generateRandomDescription â€“ najpierw normalizuj, usuÅ„ â€œhasNoProblematicStartâ€:
ZastÄ…p caÅ‚Ä… logikÄ™ po odebraniu response tak:

    console.log('ðŸ” DEBUG: Raw LLM response:', JSON.stringify(response));
    console.log('ðŸ” DEBUG: Response length:', response?.length || 0);

    const cleaned = cleanTextResponse(response);
    console.log('ðŸ” DEBUG: Cleaned response:', JSON.stringify(cleaned));
    console.log('ðŸ” DEBUG: Cleaned length:', cleaned?.length || 0);

    // ZAWSZE sprÃ³buj znormalizowaÄ‡
    const normalized = normalizeToTagsLine(cleaned);
    console.log('ðŸ” DEBUG: Normalized result:', JSON.stringify(normalized));

    if (normalized && normalized.trim().length > 0) {
      console.log('ðŸ” DEBUG: Using LLM response!');
      return normalized;
    }

    console.log('ðŸ” DEBUG: Using fallback due to empty normalized output');
    const fallbackElements = [
      ...baseElements,
      'detailed',
      'high quality',
      Math.random() > 0.5 ? 'natural lighting' : 'dramatic lighting',
      Math.random() > 0.5 ? 'shallow depth of field' : 'sharp focus',
      Math.random() > 0.5 ? 'cinematic' : 'artistic',
      'masterpiece'
    ];
    return normalizeToTagsLine(fallbackElements.join(', '));

UWAGA: PamiÄ™taj, Å¼e to wywoÅ‚anie ma korzystaÄ‡ z nowych STOP_SEQS (bez ASSISTANT).

B) main.js

1) unifyStop â€“ usuÅ„ ASSISTANT z domyÅ›lnych stopÃ³w:
ZmieÅ„:
  return ['USER:', '\nUSER:', 'ASSISTANT:', '\nASSISTANT:'];
Na:
  return ['USER:', '\nUSER:', '\n\nUSER:'];

2) trimAtStopSequences â€“ tnij tylko po USER:
ZmieÅ„ implementacjÄ™ na:
  function trimAtStopSequences(text, stops) {
    if (!text || !stops?.length) return text;
    const userStops = stops.filter(s => s.includes('USER:'));
    let idx = -1;
    for (const s of userStops) {
      const i = text.indexOf(s);
      if (i !== -1) {
        if (idx === -1 || i < idx) idx = i;
      }
    }
    return idx === -1 ? text : text.slice(0, idx);
  }

3) WywoÅ‚anie session.prompt â€“ przekaÅ¼ tylko stopSequences i antiPrompts (bez dodatkowego â€œstopâ€):
ZmieÅ„ fragment:
  const response = await session.prompt(prompt, {
    temperature,
    topP,
    topK,
    maxTokens,
    stopSequences: stop,
    stop,
    antiPrompts: stop
  });
Na:
  const response = await session.prompt(prompt, {
    temperature,
    topP,
    topK,
    maxTokens,
    stopSequences: stop,   // tylko USER:
    antiPrompts: stop      // tylko USER:
  });

4) Dodatkowe logi do diagnostyki:
Zaraz po otrzymaniu â€˜responseâ€™:
  console.log('[LLM][RAW OUT len]', response?.length, 'text=', JSON.stringify(response));
Po trymowaniu:
  const trimmed = trimAtStopSequences(response, stop);
  console.log('[LLM][POST-TRIM len]', trimmed?.length, 'text=', JSON.stringify(trimmed));
  return { result: trimmed ?? response };

C) electronService.ts
- Bez zmian funkcjonalnych. Warto tylko dopisaÄ‡ extra log w invokeLLM, Å¼eby wiedzieÄ‡ jakie stopâ€™y idÄ… do main (opcjonalnie):
  console.log('[LLM][invokeLLM] stops:', payload.stop);

Checklist szybkiej weryfikacji
- [ ] Czy w STOP_SEQS oraz unifyStop usuniÄ™to â€œASSISTANT:â€?
- [ ] Czy prompt koÅ„czy siÄ™ na â€œASSISTANT: â€ (ze spacjÄ…)?
- [ ] Czy main.js nie przycina po â€œASSISTANT:â€, a ewentualny trim robi tylko po â€œUSER:â€?
- [ ] Czy generateRandomDescription zawsze prÃ³buje normalizeToTagsLine przed fallbackiem (bez pre-filtrowania â€œHere/This/Createâ€¦â€)? 
- [ ] Czy normalizer nie psuje liczb i ekspozycji (nie zamienia kropek ani slashy)?
- [ ] Czy w logach masz: [PROMPT], [RAW OUT], [POST-TRIM], [NORMALIZED]?

Jak debugowaÄ‡ lokalny LLM (praktycznie)
- Loguj surowy prompt po zÅ‚oÅ¼eniu (JSON.stringify, Å¼eby widzieÄ‡ znaki nowej linii).
- Upewnij siÄ™, Å¼e stop reason nie odcina odpowiedzi do zera (czasem biblioteka ucina po pierwszym tokenie, gdy stop siÄ™ â€œzaciÅ›nieâ€).
- PorÃ³wnaj wynik z CLI (llama.cpp) z tymi samymi stopami i promptem; jeÅ›li CLI zwraca Å‚adne CSV, problem jest po stronie IPC/parsingu.
- Test jednostkowy dla normalizeToTagsLine na kilku przykÅ‚adach: CSV, bullet listy, JSON, tekst z â€œTags: â€¦â€, techniczne wartoÅ›ci (f/1.8, 1/125s).

Dlaczego dotÄ…d zawsze byÅ‚ fallback
- Stop zawieraÅ‚ â€œASSISTANT:â€, wiÄ™c:
  - Model bywaÅ‚ ucinany natychmiast (0 tokenÃ³w), lub
  - Post-trim usuwaÅ‚ wszystko, gdy tylko pojawiÅ‚o siÄ™ â€œASSISTANT:â€ w strumieniu.
- Dodatkowa heurystyka (hasNoProblematicStart) odrzucaÅ‚a odpowiedzi zaczynajÄ…ce siÄ™ od â€œHere/This/Createâ€, zanim w ogÃ³le je znormalizowaÅ‚eÅ›.
- Normalizer czasem rozbijaÅ‚ format za mocno.

Po wdroÅ¼eniu zmian
- Model zacznie zwracaÄ‡ treÅ›Ä‡ po â€œASSISTANT: â€, a generacja zostanie uciÄ™ta dopiero przy â€œUSER:â€.
- Normalizer bÄ™dzie dostawaÅ‚ stabilne, przewidywalne CSV i zwrÃ³ci je bez zbÄ™dnych ingerencji.
- Fallback bÄ™dzie odpalaÄ‡ tylko w realnych bÅ‚Ä™dach (pusty output, bÅ‚Ä…d IPC), nie â€œz urzÄ™duâ€.

â€” Koniec â€”