Nazwa pliku: wizardlm_random_contextsequence_fix.txt
Data: 2025-09-13
Temat: WizardLM Random – pierwsze działa, drugie zwraca "contextSequence cannot be null" (efemeryczna sesja i cykl życia sekwencji)

1) Objawy
- Pierwsze kliknięcie Random działa poprawnie (narracyjny opis).
- Drugie kliknięcie: błąd "contextSequence cannot be null".
- Używasz efemerycznej sesji: new LlamaChatSession({ context }) + session.dispose() w finally.
- context utworzony z sequences: 2.

2) Diagnoza (co się dzieje)
- W Twojej wersji node-llama-cpp konstruktor LlamaChatSession({ context }) najpewniej nie gwarantuje automatycznego wypożyczenia sekwencji z puli LUB nie oddaje jej poprawnie po dispose.
- Niektóre wersje biblioteki wymagają jawnego przekazania contextSequence do konstruktora (new LlamaChatSession({ contextSequence })). W przeciwnym razie wewnątrz sesji contextSequence pozostaje null i przy drugim wywołaniu pojawia się wyjątek.
- Alternatywnie: sesja z { context } wypożycza sekwencję poprawnie przy pierwszym użyciu, ale dispose nie zwraca jej do puli w Twojej wersji → przy kolejnym wywołaniu sesja nie ma sekwencji (null).

W praktyce oznacza to, że ścieżka „efemeryczna sesja bez jawnej sekwencji” nie jest stabilna w Twojej wersji node-llama-cpp.

3) Krótka odpowiedź (co zrobić)
- Przejdź na Opcję B: ręczne zarządzanie sekwencją per żądanie (getSequence → LlamaChatSession({ contextSequence: seq }) → po użyciu zawsze release()/dispose() sekwencji w finally).
- Alternatywnie rozważ Opcję C: model.createCompletion() (omija ChatSession i sekwencje jawnie).
- Dodatkowo podnieś sequences w createContext do 2–4 i/lub serializuj żądania (lock), aby uniknąć kolizji.

4) Odpowiedzi na Twoje pytania
1. Czy new LlamaChatSession({ context }) w mojej wersji automatycznie pobiera sekwencję?
   - Najpewniej nie w pełni (albo nie zwraca jej poprawnie po dispose). Wskazuje na to błąd „contextSequence cannot be null” przy drugim użyciu.

2. Czy wersja wymaga jawnego contextSequence w konstruktorze?
   - Wygląda na to, że tak. Bezpieczniej przejść na jawne getSequence() i przekazać contextSequence do LlamaChatSession.

3. Czy wrócić do Opcji B (ręczne getSequence + release)?
   - Tak. To najbardziej przewidywalne podejście w różnych wersjach biblioteki.

4. Czy problem może być w session.dispose() (nie zwraca sekwencji)?
   - Możliwe. W niektórych wersjach dispose nie oddaje sekwencji, jeśli sesja nie „posiada” jej ekskluzywnie lub gdy sekwencja została wstrzyknięta z zewnątrz. Dlatego zwalniaj sekwencję samodzielnie.

5. Jak sprawdzić wersję node-llama-cpp i czy konstruktor { context } jest wspierany?
   - Sprawdź wersję w terminalu: npm ls node-llama-cpp lub pnpm ls node-llama-cpp.
   - W kodzie (ESM z importem JSON, jeśli wspierane): 
     import llamaPkg from 'node-llama-cpp/package.json' assert { type: 'json' };
     console.log('[llama] node-llama-cpp version', llamaPkg.version);
   - Alternatywnie dodaj try/catch: spróbuj utworzyć sesję z { context } i jeśli poleci wyjątek, przełącz się runtime na ścieżkę z jawny getSequence(). Patrz: „Feature-detect” poniżej.

5) Zalecana implementacja (Opcja B – jawne sekwencje)
W main.js zastąp blok z efemeryczną sesją wersją z pobieraniem i zwalnianiem sekwencji per żądanie:

  ipcMain.handle('llm-request', async (_event, { task, payload }) => {
    const init = await initializeLlama();
    if (!init.success) return { error: init.error || 'Init failed' };

    const prompt = String(payload?.prompt || '').trim();
    if (!prompt) return { error: 'No prompt provided' };

    const temperature = Number.isFinite(payload?.temperature) ? payload.temperature : 0.6;
    const topP = Number.isFinite(payload?.top_p) ? payload.top_p : 0.9;
    const topK = 40;
    const maxTokens = Number.isFinite(payload?.maxTokens) ? payload.maxTokens : 256;
    const stop = Array.isArray(payload?.stop) && payload.stop.length
      ? payload.stop
      : ['USER:', '\nUSER:', '\n\nUSER:'];

    let seq, session;
    try {
      console.log('[LLM] acquiring sequence');
      seq = context.getSequence(); // wypożycz sekwencję z puli
      session = new LlamaChatSession({ contextSequence: seq });

      const start = Date.now();
      const response = await session.prompt(prompt, {
        temperature, topP, topK, maxTokens,
        stopSequences: stop,
        antiPrompts: stop
      });
      const ms = Date.now() - start;
      console.log(`[LLM] gen in ${ms}ms len=${response?.length}`);

      const trimmed = trimAtStopSequences(response, stop); // tnij tylko po USER:
      return { result: trimmed ?? response };
    } catch (e) {
      console.error('[LLM] error', e);
      return { error: e?.message || 'Unknown LLM error' };
    } finally {
      // Kolejność: najpierw sesja, potem sekwencja
      try { session?.dispose?.(); } catch (e) { console.warn('session.dispose failed', e); }
      try {
        console.log('[LLM] releasing sequence');
        if (typeof seq?.release === 'function') seq.release();
        else if (typeof seq?.dispose === 'function') seq.dispose();
      } catch (e) {
        console.warn('sequence release failed', e);
      }
    }
  });

Uwaga:
- Nie używaj clear()/reset()/setPosition() jako substytutu zwolnienia – to nie zwraca sekwencji do puli.
- Ustaw createContext({ sequences: 2–4 }) aby mieć zapas na szybkie kliknięcia lub dev StrictMode.
- Jeśli sequences=1, zastosuj prosty lock, by serializować żądania:
  let busy = false;
  ipcMain.handle('llm-request', async (_e, req) => {
    if (busy) return { error: 'LLM is busy, try again in a moment.' };
    busy = true;
    try { /* ... */ } finally { busy = false; }
  });

6) Alternatywa (Opcja C – bez ChatSession)
Jeśli nie potrzebujesz pamięci czatu, możesz pominąć LlamaChatSession zupełnie i użyć createCompletion:

  const out = await model.createCompletion({
    prompt,
    maxTokens,
    temperature,
    topP,
    stop: ['USER:', '\nUSER:', '\n\nUSER:'], // tylko USER: warianty
    stream: false
  });
  // W zależności od wersji: out lub out.text to string. Zaloguj i zwróć.

Ta ścieżka daje najmniej stanowości i często eliminuje problemy z sekwencjami.

7) Feature-detect (automatyczne przełączanie ścieżki)
Jeśli chcesz działać w wielu wersjach node-llama-cpp, możesz runtime wykryć, co jest wspierane:

  function canUseContextCtor() {
    try {
      const test = new LlamaChatSession({ context }); // może rzucić synchron.
      test?.dispose?.();
      return true;
    } catch {
      return false;
    }
  }

  // W handlerze:
  if (canUseContextCtor()) {
    // ścieżka efemeryczna (A) – jeśli działa stabilnie u Ciebie
  } else {
    // ścieżka jawnej sekwencji (B)
  }

Ja mimo wszystko rekomenduję (B) – przewidywalne i kompatybilne.

8) Checklist wdrożenia
- [ ] createContext({ sequences: 2–4, ... }) – większa pula na dev/test.
- [ ] W handlerze IPC: getSequence() per żądanie → LlamaChatSession({ contextSequence }) → session.dispose() → seq.release()/seq.dispose() w finally.
- [ ] Zero użycia clear/reset/setPosition zamiast release.
- [ ] Tylko USER: w stop sequences (bez ASSISTANT:).
- [ ] Opcjonalnie lock przy sequences=1 lub w dev.
- [ ] Logi: "[LLM] acquiring sequence" / "[LLM] releasing sequence", długości odpowiedzi i czasy.

9) Najczęstsze źródła tego błędu
- Konstruktor LlamaChatSession({ context }) nieobsługiwany lub niestabilny w Twojej wersji.
- Sesja nie oddaje sekwencji po dispose → następne wywołanie nie ma już contextSequence.
- Równoległe wywołania (double-click, StrictMode) przy sequences: 1 → pula pusta.

10) Jeżeli problem nie zniknie
- Podnieś sequences do 4 i dodaj lock (na czas testów).
- Przejdź na createCompletion (Opcja C) na próbę — jeśli działa stabilnie, winny jest cykl życia ChatSession/Sequence.
- Sprawdź wersję node-llama-cpp (npm ls node-llama-cpp) i dokumentację konstruktora ChatSession dla tej wersji.
- Uruchom identyczny prompt przez CLI llama.cpp — jeśli tam działa stale, problem na 99% leży w zarządzaniu sekwencją po stronie node bindingów.

Podsumowanie
- Błąd „contextSequence cannot be null” wskazuje, że sesja nie ma przydzielonej sekwencji przy drugim wywołaniu.
- Najpewniejsza naprawa: ręczne getSequence() + release() per żądanie (Opcja B) lub createCompletion (Opcja C).
- Unikaj clear/reset — tylko release/dispose faktycznie oddaje sekwencję do puli.

— Koniec —