# ComfyUI Prompt Builder Node

Integracja Prompt Builder z ComfyUI umo≈ºliwiajƒÖca generowanie zaawansowanych prompt√≥w przy u≈ºyciu lokalnych modeli LLM.

## üöÄ Funkcje

- **Lokalne LLM** - Obs≈Çuga lokalnych modeli AI (Ollama, Mistral, LM Studio)
- **Generowanie prompt√≥w** - Automatyczne tworzenie szczeg√≥≈Çowych prompt√≥w pozytywnych i negatywnych
- **Style** - Wsparcie dla r√≥≈ºnych styl√≥w: photorealistic, anime, artistic
- **Konfigurowalno≈õƒá** - Pe≈Çna kontrola nad parametrami generowania
- **Offline** - Dzia≈Ça ca≈Çkowicie offline z lokalnymi modelami

## üì¶ Instalacja

### 1. Przez ComfyUI Manager (Zalecane)
 
  <<<<<<< comfyui-node
1. Otw√≥rz ComfyUI
2. Kliknij "Manager" w menu
3. Wybierz "Install Custom Nodes"
4. Wklej URL: `https://github.com/btitkin/promptbuilder.git`
5. Kliknij "Install"
6. Zrestartuj ComfyUI
=======
**[üíª DESKTOP VERSION - USE WITH YOUR LOCAL LLM! ](https://github.com/btitkin/promptbuilder/tree/local_llm_version)üíª**

**[‚ñ∂Ô∏è You can try the PromptBuilder now here, enjoy it! ](https://btitkin.github.io/promptbuilder/)‚óÄÔ∏è**
>>>>>>> main

### 2. Instalacja manualna

1. Przejd≈∫ do folderu `ComfyUI/custom_nodes/`
2. Sklonuj repozytorium:
   ```bash
   git clone https://github.com/btitkin/promptbuilder.git comfyui-promptbuilder-node
   ```
3. Zainstaluj zale≈ºno≈õci:
   ```bash
   cd comfyui-promptbuilder-node
   pip install -r requirements.txt
   ```
4. Zrestartuj ComfyUI

## üîß Konfiguracja lokalnego LLM

### Ollama
```bash
# Instalacja Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Pobranie modelu
ollama pull mistral

# Uruchomienie serwera
ollama serve
```

### LM Studio
1. Pobierz i zainstaluj LM Studio
2. Pobierz model (np. Mistral 7B)
3. Uruchom lokalny serwer na porcie 1234

### Inne kompatybilne API
Wszystkie API kompatybilne z OpenAI (LocalAI, text-generation-webui, itp.)

## üéØ U≈ºytkowanie

### Podstawowe u≈ºycie

1. **Dodaj node** - W ComfyUI dodaj node "Prompt Builder (Local LLM)"
2. **Konfiguruj po≈ÇƒÖczenie:**
   - **API URL**: `http://127.0.0.1:1234` (dla LM Studio)
   - **Model Name**: `mistral` lub nazwa twojego modelu
3. **Wprowad≈∫ opis**: "A beautiful sunset over mountains"
4. **Wybierz styl**: photorealistic/anime/artistic
5. **Uruchom** - Node wygeneruje enhanced prompty

### Parametry wej≈õciowe

| Parametr | Typ | Domy≈õlna | Opis |
|----------|-----|----------|------|
| `description` | STRING | "A beautiful sunset..." | Podstawowy opis sceny |
| `api_url` | STRING | "http://127.0.0.1:1234" | URL lokalnego API LLM |
| `model_name` | STRING | "dolphin-2.7-mixtral-8x7b" | Nazwa modelu |
| `style_filter` | CHOICE | "photorealistic" | Styl: photorealistic/anime/artistic |
| `num_variations` | INT | 3 | Liczba wariant√≥w (1-10) |
| `api_key` | STRING | "" | Klucz API (opcjonalny) |
| `temperature` | FLOAT | 0.7 | Kreatywno≈õƒá (0.1-2.0) |
| `max_tokens` | INT | 2000 | Maksymalna d≈Çugo≈õƒá odpowiedzi |

### Wyj≈õcia

- **positive_prompt** - Szczeg√≥≈Çowy prompt pozytywny
- **negative_prompt** - Prompt negatywny (elementy do unikania)
- **enhanced_description** - Rozszerzony opis sceny

## üîó Przyk≈Çadowy workflow

```
[Prompt Builder Node] ‚Üí [CLIP Text Encode] ‚Üí [KSampler] ‚Üí [VAE Decode] ‚Üí [Save Image]
                    ‚Üò [CLIP Text Encode (Negative)]
```

## ‚öôÔ∏è Konfiguracja zaawansowana

### R√≥≈ºne modele LLM

**Ollama:**
- URL: `http://127.0.0.1:11434`
- Modele: `mistral`, `llama2`, `codellama`

**LM Studio:**
- URL: `http://127.0.0.1:1234`
- Modele: Dowolny za≈Çadowany model

**text-generation-webui:**
- URL: `http://127.0.0.1:5000`
- Tryb: OpenAI API compatibility

### Optymalizacja prompt√≥w

**Photorealistic:**
- Dodaje terminy fotograficzne
- Skupia siƒô na o≈õwietleniu i kompozycji
- Zawiera szczeg√≥≈Çy techniczne

**Anime:**
- U≈ºywa terminologii anime/manga
- Dodaje elementy stylu artystycznego
- Zawiera referencje do designu postaci

**Artistic:**
- Skupia siƒô na ruchach artystycznych
- Dodaje techniki malarskie
- Zawiera elementy estetyczne

## üêõ RozwiƒÖzywanie problem√≥w

### "Connection Error"
- Sprawd≈∫ czy lokalny LLM jest uruchomiony
- Zweryfikuj URL API (http://127.0.0.1:1234)
- Sprawd≈∫ czy port nie jest zablokowany

### "API Error 404"
- Sprawd≈∫ czy model jest za≈Çadowany
- Zweryfikuj nazwƒô modelu
- Sprawd≈∫ czy API endpoint jest poprawny

### "Timeout Error"
- Zwiƒôksz timeout w kodzie node
- Sprawd≈∫ wydajno≈õƒá systemu
- Rozwa≈º mniejszy model LLM

### CORS Issues (dla aplikacji webowych)
- U≈ºyj aplikacji desktop (Electron)
- Skonfiguruj proxy w serwerze deweloperskim
- Uruchom LLM z obs≈ÇugƒÖ CORS

## üìã Wymagania systemowe

- **ComfyUI** - Najnowsza wersja
- **Python** - 3.8+
- **RAM** - Min. 8GB (16GB zalecane dla wiƒôkszych modeli)
- **GPU** - Opcjonalne (dla przyspieszenia LLM)
- **Lokalny LLM** - Ollama/LM Studio/inne

## ü§ù Wsparcie

- **Issues**: [GitHub Issues](https://github.com/btitkin/promptbuilder/issues)
- **Dokumentacja**: [Wiki](https://github.com/btitkin/promptbuilder/wiki)
- **Dyskusje**: [GitHub Discussions](https://github.com/btitkin/promptbuilder/discussions)

## üìÑ Licencja

MIT License - Zobacz [LICENSE](LICENSE) dla szczeg√≥≈Ç√≥w.

## üôè Podziƒôkowania

- ComfyUI team za fantastycznƒÖ platformƒô
- Spo≈Çeczno≈õƒá AI za wsparcie i feedback
- Tw√≥rcy lokalnych modeli LLM

---

**Prompt Builder ComfyUI Node** - Generuj lepsze prompty z mocƒÖ lokalnych LLM! üöÄ
