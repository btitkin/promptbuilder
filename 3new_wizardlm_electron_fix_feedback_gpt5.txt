Jasne! Skopiuj cały blok poniżej do pliku: wizardlm_random_sequences_fix.txt

```text
Nazwa pliku: wizardlm_random_sequences_fix.txt
Data: 2025-09-13
Temat: WizardLM Random – pierwsze generowanie działa, kolejne zwraca "No sequences left" (zarządzanie sekwencjami w node-llama-cpp)

1) Objawy
- Pierwsze kliknięcie Random działa poprawnie.
- Drugie i kolejne: błąd "No sequences left".
- W logach: Local LLM error: Error: No sequences left at invokeLLM (...)

2) Diagnoza (dlaczego tak się dzieje)
- Korzystasz z context.createContext({ sequences: 1 }) i ręcznie zarządzasz contextSequence.
- clear()/reset()/setPosition(0) NIE zwracają sekwencji do puli. To nie to samo co release/dispose.
- W else wywołujesz context.getSequence(), co przy sequences: 1 powoduje “zużycie” jedynej sekwencji. Po pierwszym użyciu kolejny getSequence prowadzi do "No sequences left".
- Jeśli session.dispose() nie zwraca sekwencji (zwłaszcza gdy przekazujesz własny contextSequence), pula zostaje pusta.

3) Zasada ogólna
- Sekwencję pobieramy per żądanie i ZAWSZE zwracamy do puli (release/dispose) w bloku finally.
- Nie używamy clear/reset jako substytutu release.
- Najprościej: nie zarządzać sekwencją ręcznie (efemeryczna sesja) lub w ogóle ominąć ChatSession i użyć createCompletion.

4) Zalecane rozwiązania (wybierz jedno)

Opcja A: Efemeryczna sesja (bez jawnej sekwencji) – najprostsza
- Twórz LlamaChatSession na każde żądanie i ją dispose’uj. W wielu wersjach node-llama-cpp konstruktor przyjmujący context sam wypożycza/oddaje sekwencję.
- Jeśli Twoja wersja nie wspiera new LlamaChatSession({ context }), skorzystaj z opcji B.

Przykład (main.js):
  // globalnie trzymaj tylko: llama, model, context
  let llama, model, context;

  async function initializeLlama() {
    if (context) return { success: true };
    llama = await getLlama();
    model = await llama.loadModel({
      modelPath,
      gpuLayers: 'max',
      defaultContextFlashAttention: true
    });

    const cpuThreads = Math.max(4, Math.min(16, (os.availableParallelism?.() ?? os.cpus().length) - 1));

    context = await model.createContext({
      contextSize: 512,
      sequences: 2, // daj 2–4 aby przeżyć szybkie podwójne kliknięcia/dev StrictMode
      threads: cpuThreads,
      flashAttention: true
    });
    return { success: true };
  }

  ipcMain.handle('llm-request', async (_event, { task, payload }) => {
    const init = await initializeLlama();
    if (!init.success) return { error: init.error || 'Init failed' };

    const prompt = String(payload?.prompt || '').trim();
    if (!prompt) return { error: 'No prompt provided' };

    const temperature = Number.isFinite(payload?.temperature) ? payload.temperature : 0.6;
    const topP = Number.isFinite(payload?.top_p) ? payload.top_p : 0.9;
    const topK = 40;
    const maxTokens = Number.isFinite(payload?.maxTokens) ? payload.maxTokens : 256;
    const stop = Array.isArray(payload?.stop) && payload.stop.length
      ? payload.stop
      : ['USER:', '\nUSER:', '\n\nUSER:'];

    let session;
    try {
      session = new LlamaChatSession({ context }); // jeśli Twoja wersja to wspiera
      const start = Date.now();
      const response = await session.prompt(prompt, {
        temperature, topP, topK, maxTokens,
        stopSequences: stop,
        antiPrompts: stop
      });
      const ms = Date.now() - start;
      console.log(`[LLM] gen in ${ms}ms len=${response?.length}`);

      const trimmed = trimAtStopSequences(response, stop); // tnij tylko po USER:
      return { result: trimmed ?? response };
    } catch (e) {
      console.error('LLM error:', e);
      return { error: e?.message || 'Unknown LLM error' };
    } finally {
      try { session?.dispose?.(); } catch (e) { console.warn('session.dispose failed', e); }
    }
  });

Opcja B: Ręczne pobieranie i ZWRACANIE sekwencji per żądanie – pełna kontrola
- Pobierz sekwencję context.getSequence(), stwórz sesję na tej sekwencji, po użyciu ZAWSZE zwolnij: seq.release()/seq.dispose() (w zależności od wersji).
- Nie używaj clear/reset jako zamiennika zwolnienia.

  ipcMain.handle('llm-request', async (_event, { task, payload }) => {
    await initializeLlama();

    const prompt = String(payload?.prompt || '').trim();
    const stop = Array.isArray(payload?.stop) && payload.stop.length
      ? payload.stop
      : ['USER:', '\nUSER:', '\n\nUSER:'];

    let seq, session;
    try {
      console.log('[LLM] acquiring sequence');
      seq = context.getSequence(); // wypożycz sekwencję z puli
      session = new LlamaChatSession({ contextSequence: seq });

      const response = await session.prompt(prompt, {
        temperature: 0.8, topP: 0.95, topK: 40, maxTokens: 200,
        stopSequences: stop, antiPrompts: stop
      });

      const trimmed = trimAtStopSequences(response, stop);
      return { result: trimmed ?? response };
    } catch (e) {
      return { error: e?.message || 'LLM error' };
    } finally {
      // Kolejność: najpierw sesja, potem sekwencja
      try { session?.dispose?.(); } catch {}
      try {
        console.log('[LLM] releasing sequence');
        if (typeof seq?.release === 'function') seq.release();
        else if (typeof seq?.dispose === 'function') seq.dispose();
        // Nie używaj clear/reset jako substytutu release
      } catch (e) {
        console.warn('sequence release failed', e);
      }
    }
  });

Opcja C: createCompletion bez ChatSession – najprostsza kontrola
- Omiń ChatSession i zarządzanie sekwencją; node-llama-cpp zwykle sam ogarnia pod spodem.

  const response = await model.createCompletion({
    prompt,
    maxTokens,
    temperature,
    topP,
    stop: ['USER:', '\nUSER:', '\n\nUSER:'], // tylko USER: warianty
    stream: false
  });
  // w zależności od wersji: response.text lub string jako wynik

5) Dodatkowe zabezpieczenia
- Ustaw sequences: 2–4 w createContext, żeby:
  - przetrwać szybkie podwójne kliknięcia,
  - poradzić sobie z React StrictMode (potrafi uruchomić efekt 2x w dev).
- Serializacja żądań (jeśli sequences=1):
  let busy = false;
  ipcMain.handle('llm-request', async (_e, req) => {
    if (busy) return { error: 'LLM is busy, try again in a moment.' };
    busy = true;
    try { /* ... */ } finally { busy = false; }
  });
- Usuń wszelki “reset sekwencji” (clear/reset/setPosition) – to nie zwalnia puli.
- Nie trzymaj globalnie LlamaChatSession ani contextSequence. Lepszy wzorzec: sesja/sekwencja “na żądanie” + finally { release }.
- Stop sequences: tylko USER: warianty (bez ASSISTANT:).
- TheBloke prompt: bez preamble, z trailing space po "ASSISTANT: ":
  "USER: ...\nASSISTANT: "

6) Odpowiedzi na Twoje pytania
1. Czy clear()/reset() niszczy sekwencję?
   - Nie “niszczy”, ale też NIE zwraca jej do puli. Nie używaj tego zamiast release/dispose.

2. Czy zawsze tworzyć nową sekwencję?
   - Per żądanie “pożyczaj” nową (getSequence) i na końcu JĄ ZWRACAJ (release/dispose). Nie recyklinguj jej resetami.

3. Czy session.dispose() wpływa na contextSequence?
   - Jeśli sesja sama wypożyczyła sekwencję, dispose zwykle ją oddaje.
   - Jeśli TY wstrzyknąłeś własną sekwencję (contextSequence), nie zakładaj, że session.dispose() ją zwróci — zwolnij ją jawnie (seq.release()/seq.dispose()).

4. Jak poprawnie zarządzać cyklem życia contextSequence?
   - Inicjalizacja: model + context (sequences >= 2 preferowane).
   - Dla każdego żądania: getSequence → użyj → always release w finally.
   - Nie używaj clear/reset/setPosition zamiast release.

5. Czy lepiej użyć createCompletion?
   - Jeśli nie potrzebujesz pamięci czatu, createCompletion daje pełną kontrolę i minimalizuje ryzyko “wycieków” sekwencji.

7) Minimalny “diff” mentalny dla Twojego main.js
- Usuń globalne contextSequence i wszelkie clear/reset/setPosition.
- Ustaw sequences: 2 (lub 4) przy createContext.
- W handlerze IPC:
  - (Opcja A) Użyj new LlamaChatSession({ context }) i session.dispose() w finally.
  - (Opcja B) Pobierz seq = context.getSequence(), stwórz sesję, po wszystkim session.dispose(); seq.release()/seq.dispose() w finally.
- Zachowaj stop sequences tylko dla USER:.

8) Checklist wdrożenia
- [ ] createContext({ sequences: 2–4, ... })
- [ ] Brak globalnego contextSequence; brak clear/reset/setPosition.
- [ ] Per żądanie: efemeryczna sesja lub jawne getSequence + release w finally.
- [ ] Stop sequences: ['USER:', '\nUSER:', '\n\nUSER:'] (bez 'ASSISTANT:').
- [ ] TheBloke prompt bez preamble, z "ASSISTANT: " (spacja).
- [ ] Dodatkowo: lock przy sequences=1 lub w dev.

9) Najczęstsze pułapki
- Dev React StrictMode = podwójne wywołania efektów → dwa równoległe zapytania.
- Manualne context.getSequence bez późniejszego release → “No sequences left”.
- Mylenie release z clear/reset → pula się nie opróżnia.
- Równoległe kliknięcia przy sequences=1 → konflikt.

10) Diagnostyka (logi)
- Przed getSequence: console.log('[LLM] acquiring sequence');
- Po zwolnieniu: console.log('[LLM] releasing sequence');
- Loguj też: [PROMPT len], [RAW OUT len], [POST-TRIM len] dla szybkiej weryfikacji przepływu.

Po zastosowaniu któregoś z powyższych podejść każde kliknięcie Random będzie niezależne i nie “zużyje” puli sekwencji. Jeśli chcesz, mogę przygotować gotowy plik main.js na bazie opcji A lub C dopasowany do Twojej wersji node-llama-cpp.
— Koniec —
```